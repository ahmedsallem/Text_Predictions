{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hgzvmjndhwo"
      },
      "source": [
        "#**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsnBjEs-boId"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qEovcJTaWCX",
        "outputId": "d1c87e5d-fabb-462d-c9ed-35324acbffc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import keras # main keras package\n",
        "from keras.models import Sequential # sequential model\n",
        "from keras.layers import Dropout, Flatten, AveragePooling2D # layers with layers operations\n",
        "from keras.layers import Dense,Conv2D  # layers types\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "print(\"Keras version:\", keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF1bPRiTNRYT"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os, time\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import applications\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D , Embedding , LSTM, GRU, Bidirectional,Dropout, SpatialDropout2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten , AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL72iL44yNLu"
      },
      "source": [
        "# **Load Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xiu8-vdC7xnT"
      },
      "source": [
        "The first step is to load the text into memory.\n",
        "\n",
        "We can develop a small function to load the entire text file into memory and return it. The function is called load_doc() and is listed below. Given a filename, it returns a sequence of loaded text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3EQbxQGxBqR"
      },
      "outputs": [],
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BqiMdnR724o"
      },
      "source": [
        "Using this function, we can load the cleaner version of the document in the file ‘republic_clean.txt‘ as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQBw-R2CxQBY",
        "outputId": "c5f71868-472e-4b3f-dee7-160cc003dc49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK I.\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up\n"
          ]
        }
      ],
      "source": [
        "# load document\n",
        "in_filename = 'republic_clean.txt'\n",
        "doc = load_doc(in_filename)\n",
        "print(doc[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3LAVjTp77fU"
      },
      "source": [
        "Running this snippet loads the document and prints the first 100 characters as a sanity check.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1IYn9p8yclh"
      },
      "source": [
        "# **Clean Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qktyH2oZ7_6d"
      },
      "source": [
        "We need to transform the raw text into a sequence of tokens or words that we can use as a source to train the model.\n",
        "\n",
        "Based on reviewing the raw text (above), below are some specific operations we will perform to clean the text. You may want to explore more cleaning operations yourself as an extension.\n",
        "*  Replace ‘--‘ with a white space so we can split words better.\n",
        "  \n",
        "* Split words based on white space.\n",
        "* Remove all punctuation from words to reduce the vocabulary size .\n",
        "* Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
        "* Normalize all words to lowercase to reduce the vocabulary size.\n",
        "\n",
        "Vocabulary size is a big deal with language modeling. A smaller vocabulary results in a smaller model that trains faster.\n",
        "\n",
        "We can implement each of these cleaning operations in this order in a function. Below is the function clean_doc() that takes a loaded document as an argument and returns an array of clean tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl3XzmrtyiRw"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w86poIUr8kME"
      },
      "source": [
        "We can run this cleaning function on our loaded document and print out some of the tokens and statistics as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijqZWn0DylAr",
        "outputId": "d3d1a409-5533-45ff-975c-e5ee8e629a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight']\n",
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ]
        }
      ],
      "source": [
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:100])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fji-zRs38ob0"
      },
      "source": [
        "First, we can see a  list of tokens that look cleaner than the raw text. We could remove the ‘Book I‘ chapter markers and more, but this is a good start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0-PE6Bty0-W"
      },
      "source": [
        "# **Save Clean Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBl6dFSM8xk0"
      },
      "source": [
        "We can organize the long list of tokens into sequences of 50 input words and 1 output word.\n",
        "\n",
        "That is, sequences of 51 words.\n",
        "\n",
        "We can do this by iterating over the list of tokens from token 51 onwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of the list of tokens.\n",
        "\n",
        "We will transform the tokens into space-separated strings for later storage in a file.\n",
        "\n",
        "The code to split the list of clean tokens into sequences with a length of 51 tokens is listed below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo7-ryHTyqbL",
        "outputId": "c0ab54da-d793-4ab4-c266-11c255fcdb91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 118633\n"
          ]
        }
      ],
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7879RCO157l",
        "outputId": "09a25831-9119-4da6-9142-72af6f8ebf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118633"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJMBEi6W82NY"
      },
      "source": [
        "Running this piece creates a long list of lines.\n",
        "\n",
        "Printing statistics on the list, we can see that we will have exactly 118,633 training patterns to fit our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzUg9m-788JF"
      },
      "source": [
        "Next, we can save the sequences to a new file for later loading.\n",
        "\n",
        "We can define a new function for saving lines of text to a file. This new function is called save_doc() and is listed below. It takes as input a list of lines and a filename. The lines are written, one per line, in ASCII format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNA4n6SazASs"
      },
      "outputs": [],
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBYVFpwR9AWy"
      },
      "source": [
        "We can call this function and save our training sequences to the file ‘republic_sequences.txt‘.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShEzTAPXzH6d"
      },
      "outputs": [],
      "source": [
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "774BvdCAzaps"
      },
      "source": [
        "# **Train Language Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdghdUHm9IR7"
      },
      "source": [
        "We can now train a statistical language model from the prepared data.\n",
        "\n",
        "The model we will train is a neural language model. It has a few unique characteristics:\n",
        "* It uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
        "* It learns the representation at the same time as learning the model.\n",
        "* It learns to predict the probability for the next word using the context of the last 100 words.\n",
        "\n",
        "Specifically, we will use an Embedding Layer to learn the representation of words, and a Long Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on their context.\n",
        "\n",
        "Let’s start by loading our training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKusMwYkzjKt"
      },
      "source": [
        "**Load Sequences**\n",
        "\n",
        "\n",
        "We can load our training data using the load_doc() function we developed in the previous section.\n",
        "\n",
        "Once loaded, we can split the data into separate training sequences by splitting based on new lines.\n",
        "\n",
        "The snippet below will load the ‘republic_sequences.txt‘ data file from the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H0m0EDDzP5L"
      },
      "outputs": [],
      "source": [
        "# # load doc into memory\n",
        "# def load_doc(filename):\n",
        "# \t# open the file as read only\n",
        "# \tfile = open(filename, 'r')\n",
        "# \t# read all text\n",
        "# \ttext = file.read()\n",
        "# \t# close the file\n",
        "# \tfile.close()\n",
        "# \treturn text\n",
        " \n",
        "# load document\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM49Tu4t9eVr"
      },
      "source": [
        "Next, we can encode the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1D6QWd2ztl7"
      },
      "source": [
        "**Encode Sequences**\n",
        "\n",
        "The word embedding layer expects input sequences to be comprised of integers.\n",
        "\n",
        "We can map each word in our vocabulary to a unique integer and encode our input sequences. Later, when we make predictions, we can convert the prediction to numbers and look up their associated words in the same mapping.\n",
        "\n",
        "To do this encoding, we will use the Tokenizer class in the Keras API.\n",
        "\n",
        "First, the Tokenizer must be trained on the entire training dataset, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
        "\n",
        "We can then use the fit Tokenizer to encode all of the training sequences, converting each sequence from a list of words to a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_4y2Mswzo9R"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display first line after encoding\n",
        "print(sequences[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmUyKqOosV-V",
        "outputId": "6e1d0e82-e5e1-4165-c721-db030b9c6c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 1045, 329, 7409, 4, 1, 2873, 35, 213, 1, 261, 3, 2251, 9, 11, 179, 817, 123, 92, 2872, 4, 1, 2249, 7408, 1, 7407, 7406, 2, 75, 120, 11, 1266, 4, 110, 6, 30, 168, 16, 49, 7405, 1, 1609, 13, 57, 8, 549, 151, 11, 57, 1147, 35]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAkaho6j9vA3"
      },
      "source": [
        "We can access the mapping of words to integers as a dictionary attribute called word_index on the Tokenizer object.\n",
        "\n",
        "We need to know the size of the vocabulary for defining the embedding layer later. We can determine the vocabulary by calculating the size of the mapping dictionary.\n",
        "\n",
        "Words are assigned values from 1 to the total number of words (e.g. 7,409). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the word at the end of the vocabulary will be 7,409; that means the array must be 7,409 + 1 in length.\n",
        "\n",
        "Therefore, when specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than the actual vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOlFYmnBzzRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252f976d-d306-4adf-cbb9-d87f29c23b55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7410"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2bXsWYB1pp1"
      },
      "source": [
        "# **Sequence Inputs and Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msLF0mnF1jKE"
      },
      "outputs": [],
      "source": [
        "# separate into input and output\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1] #50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Models**"
      ],
      "metadata": {
        "id": "3-isxOyRjfgT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYK6Uzip9WWk"
      },
      "source": [
        "## **Fit LSTM Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UuSnBQj9WWq"
      },
      "source": [
        "We can now define and fit our language model on the training data.\n",
        "\n",
        "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
        "\n",
        "\n",
        "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
        "\n",
        "A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufEH-kq39WWz"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "LSTM_model.add(LSTM(100, return_sequences=True))\n",
        "LSTM_model.add(LSTM(100))\n",
        "LSTM_model.add(Dense(100, activation='relu'))\n",
        "LSTM_model.add(Dense(vocab_size, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2sfSZ4P9WW2"
      },
      "source": [
        "A summary of the defined network is printed as a sanity check to ensure we have constructed what we intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d28e146-69d7-4a1e-f9f2-8c1cfdfa1287",
        "id": "bWVsI18T9WW3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 50)            370500    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 100)           60400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7410)              748410    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,269,810\n",
            "Trainable params: 1,269,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(LSTM_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfJGV0ub9WW6"
      },
      "source": [
        "Next, the model is compiled specifying the categorical cross entropy loss needed to fit the model. Technically, the model is learning a multi-class classification and this is the suitable loss function for this type of problem. The efficient Adam implementation to mini-batch gradient descent is used and accuracy is evaluated of the model.\n",
        "\n",
        "Finally, the model is fit on the data for 100 training epochs with a modest batch size of 128 to speed things up.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8567151f-2fdb-41f2-8301-f1e11d65692c",
        "id": "1iGiyGBL9WW8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "396/396 [==============================] - 13s 19ms/step - loss: 6.3140 - accuracy: 0.0604\n",
            "Epoch 2/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.8767 - accuracy: 0.0908\n",
            "Epoch 3/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.6503 - accuracy: 0.1077\n",
            "Epoch 4/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.4634 - accuracy: 0.1274\n",
            "Epoch 5/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.3315 - accuracy: 0.1378\n",
            "Epoch 6/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 5.2355 - accuracy: 0.1464\n",
            "Epoch 7/100\n",
            "396/396 [==============================] - 8s 20ms/step - loss: 5.1574 - accuracy: 0.1534\n",
            "Epoch 8/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 5.0913 - accuracy: 0.1583\n",
            "Epoch 9/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 5.0342 - accuracy: 0.1621\n",
            "Epoch 10/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.9816 - accuracy: 0.1654\n",
            "Epoch 11/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.9343 - accuracy: 0.1669\n",
            "Epoch 12/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.8873 - accuracy: 0.1692\n",
            "Epoch 13/100\n",
            "396/396 [==============================] - 8s 19ms/step - loss: 4.8413 - accuracy: 0.1711\n",
            "Epoch 14/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.7978 - accuracy: 0.1732\n",
            "Epoch 15/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.7522 - accuracy: 0.1760\n",
            "Epoch 16/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.7072 - accuracy: 0.1777\n",
            "Epoch 17/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.6637 - accuracy: 0.1804\n",
            "Epoch 18/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.6194 - accuracy: 0.1826\n",
            "Epoch 19/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.5797 - accuracy: 0.1843\n",
            "Epoch 20/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.5550 - accuracy: 0.1851\n",
            "Epoch 21/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.5029 - accuracy: 0.1877\n",
            "Epoch 22/100\n",
            "396/396 [==============================] - 8s 19ms/step - loss: 4.4641 - accuracy: 0.1915\n",
            "Epoch 23/100\n",
            "396/396 [==============================] - 8s 20ms/step - loss: 4.4294 - accuracy: 0.1927\n",
            "Epoch 24/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.3909 - accuracy: 0.1951\n",
            "Epoch 25/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.3562 - accuracy: 0.1972\n",
            "Epoch 26/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.3218 - accuracy: 0.1996\n",
            "Epoch 27/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.2887 - accuracy: 0.2017\n",
            "Epoch 28/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.2551 - accuracy: 0.2040\n",
            "Epoch 29/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.2281 - accuracy: 0.2055\n",
            "Epoch 30/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.1949 - accuracy: 0.2076\n",
            "Epoch 31/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.1676 - accuracy: 0.2092\n",
            "Epoch 32/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.1387 - accuracy: 0.2121\n",
            "Epoch 33/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.1108 - accuracy: 0.2143\n",
            "Epoch 34/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.0847 - accuracy: 0.2158\n",
            "Epoch 35/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.0588 - accuracy: 0.2194\n",
            "Epoch 36/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.0356 - accuracy: 0.2208\n",
            "Epoch 37/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 4.0107 - accuracy: 0.2223\n",
            "Epoch 38/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.9889 - accuracy: 0.2248\n",
            "Epoch 39/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.9652 - accuracy: 0.2264\n",
            "Epoch 40/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.9424 - accuracy: 0.2280\n",
            "Epoch 41/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.9214 - accuracy: 0.2315\n",
            "Epoch 42/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.9019 - accuracy: 0.2328\n",
            "Epoch 43/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.8812 - accuracy: 0.2353\n",
            "Epoch 44/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.8605 - accuracy: 0.2369\n",
            "Epoch 45/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.8428 - accuracy: 0.2392\n",
            "Epoch 46/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.8217 - accuracy: 0.2416\n",
            "Epoch 47/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.8027 - accuracy: 0.2437\n",
            "Epoch 48/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.7857 - accuracy: 0.2459\n",
            "Epoch 49/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.7674 - accuracy: 0.2473\n",
            "Epoch 50/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.7504 - accuracy: 0.2487\n",
            "Epoch 51/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.7319 - accuracy: 0.2513\n",
            "Epoch 52/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.7152 - accuracy: 0.2529\n",
            "Epoch 53/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.6978 - accuracy: 0.2554\n",
            "Epoch 54/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.6824 - accuracy: 0.2580\n",
            "Epoch 55/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.6662 - accuracy: 0.2592\n",
            "Epoch 56/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.6505 - accuracy: 0.2603\n",
            "Epoch 57/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.6351 - accuracy: 0.2629\n",
            "Epoch 58/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.6192 - accuracy: 0.2647\n",
            "Epoch 59/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.6024 - accuracy: 0.2670\n",
            "Epoch 60/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.5876 - accuracy: 0.2684\n",
            "Epoch 61/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.5734 - accuracy: 0.2706\n",
            "Epoch 62/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.5586 - accuracy: 0.2716\n",
            "Epoch 63/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.5446 - accuracy: 0.2742\n",
            "Epoch 64/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.5284 - accuracy: 0.2759\n",
            "Epoch 65/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.5141 - accuracy: 0.2792\n",
            "Epoch 66/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.4999 - accuracy: 0.2803\n",
            "Epoch 67/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.4865 - accuracy: 0.2818\n",
            "Epoch 68/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.4716 - accuracy: 0.2843\n",
            "Epoch 69/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.4578 - accuracy: 0.2857\n",
            "Epoch 70/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.4439 - accuracy: 0.2887\n",
            "Epoch 71/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.4310 - accuracy: 0.2895\n",
            "Epoch 72/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.4191 - accuracy: 0.2915\n",
            "Epoch 73/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.4048 - accuracy: 0.2937\n",
            "Epoch 74/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.3906 - accuracy: 0.2953\n",
            "Epoch 75/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.3786 - accuracy: 0.2971\n",
            "Epoch 76/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.3648 - accuracy: 0.2983\n",
            "Epoch 77/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.3526 - accuracy: 0.3003\n",
            "Epoch 78/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.3386 - accuracy: 0.3023\n",
            "Epoch 79/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.3282 - accuracy: 0.3046\n",
            "Epoch 80/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.3143 - accuracy: 0.3061\n",
            "Epoch 81/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.3013 - accuracy: 0.3082\n",
            "Epoch 82/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.2893 - accuracy: 0.3095\n",
            "Epoch 83/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.2769 - accuracy: 0.3123\n",
            "Epoch 84/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.2659 - accuracy: 0.3132\n",
            "Epoch 85/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.2516 - accuracy: 0.3161\n",
            "Epoch 86/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.2420 - accuracy: 0.3179\n",
            "Epoch 87/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.2298 - accuracy: 0.3185\n",
            "Epoch 88/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.2188 - accuracy: 0.3213\n",
            "Epoch 89/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.2050 - accuracy: 0.3234\n",
            "Epoch 90/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1918 - accuracy: 0.3242\n",
            "Epoch 91/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1838 - accuracy: 0.3267\n",
            "Epoch 92/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1704 - accuracy: 0.3273\n",
            "Epoch 93/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1573 - accuracy: 0.3298\n",
            "Epoch 94/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1489 - accuracy: 0.3310\n",
            "Epoch 95/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1362 - accuracy: 0.3345\n",
            "Epoch 96/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1254 - accuracy: 0.3354\n",
            "Epoch 97/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1139 - accuracy: 0.3371\n",
            "Epoch 98/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.1005 - accuracy: 0.3386\n",
            "Epoch 99/100\n",
            "396/396 [==============================] - 7s 19ms/step - loss: 3.0920 - accuracy: 0.3405\n",
            "Epoch 100/100\n",
            "396/396 [==============================] - 8s 19ms/step - loss: 3.0823 - accuracy: 0.3423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7effd1ab3f90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# compile model\n",
        "LSTM_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "LSTM_model.fit(X, y, batch_size=300, epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyuwmp9X9WXA"
      },
      "source": [
        "### **Save Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LmdPejv9WXB"
      },
      "source": [
        "At the end of the run, the trained model is saved to file.\n",
        "\n",
        "Here, we use the Keras model API to save the model to the file ‘model.h5‘ in the current working directory.\n",
        "\n",
        "Later, when we load the model to make predictions, we will also need the mapping of words to integers. This is in the Tokenizer object, and we can save that too using Pickle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62R-MDxM9WXD"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "# save the model to file\n",
        "LSTM_model.save('LSTM_model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clear session\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "T1xto4OX9wtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZld9HlVndBB"
      },
      "source": [
        "## **Fit GRU Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fv-xpCMndBD"
      },
      "source": [
        "We can now define and fit our language model on the training data.\n",
        "\n",
        "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
        "\n",
        "\n",
        "We will use a two GRU hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
        "\n",
        "A dense fully connected layer with 100 neurons connects to the GRU hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3x4F-8zndBG"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "GRU_model = Sequential()\n",
        "GRU_model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "GRU_model.add(GRU(100, return_sequences=True))\n",
        "GRU_model.add(GRU(100))\n",
        "GRU_model.add(Dense(100, activation='relu'))\n",
        "GRU_model.add(Dense(vocab_size, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e8UmQGHndBH"
      },
      "source": [
        "A summary of the defined network is printed as a sanity check to ensure we have constructed what we intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d564a80a-8953-4c1c-ef69-fd5a75b8c055",
        "id": "qJCm6IXondBK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 50)            370500    \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 50, 100)           45600     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 100)               60600     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7410)              748410    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,235,210\n",
            "Trainable params: 1,235,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(GRU_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWGoIGg3ndBM"
      },
      "source": [
        "Next, the model is compiled specifying the categorical cross entropy loss needed to fit the model. Technically, the model is learning a multi-class classification and this is the suitable loss function for this type of problem. The efficient Adam implementation to mini-batch gradient descent is used and accuracy is evaluated of the model.\n",
        "\n",
        "Finally, the model is fit on the data for 100 training epochs with a modest batch size of 300 to speed things up.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410ef2d2-f613-4d7c-b42e-1fd61724b995",
        "id": "KeIVMaWfndBN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "396/396 [==============================] - 12s 18ms/step - loss: 6.3576 - accuracy: 0.0576\n",
            "Epoch 2/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 6.1412 - accuracy: 0.0597\n",
            "Epoch 3/100\n",
            "396/396 [==============================] - 8s 19ms/step - loss: 6.1373 - accuracy: 0.0595\n",
            "Epoch 4/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 6.1352 - accuracy: 0.0592\n",
            "Epoch 5/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 6.1207 - accuracy: 0.0600\n",
            "Epoch 6/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.9029 - accuracy: 0.0862\n",
            "Epoch 7/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.6351 - accuracy: 0.1153\n",
            "Epoch 8/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.4319 - accuracy: 0.1382\n",
            "Epoch 9/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.2754 - accuracy: 0.1537\n",
            "Epoch 10/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.1386 - accuracy: 0.1625\n",
            "Epoch 11/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 5.0127 - accuracy: 0.1726\n",
            "Epoch 12/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.8934 - accuracy: 0.1800\n",
            "Epoch 13/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.7808 - accuracy: 0.1871\n",
            "Epoch 14/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.6793 - accuracy: 0.1927\n",
            "Epoch 15/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.5818 - accuracy: 0.1992\n",
            "Epoch 16/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.4891 - accuracy: 0.2039\n",
            "Epoch 17/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.3987 - accuracy: 0.2092\n",
            "Epoch 18/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.3072 - accuracy: 0.2138\n",
            "Epoch 19/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.2138 - accuracy: 0.2179\n",
            "Epoch 20/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.1187 - accuracy: 0.2243\n",
            "Epoch 21/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 4.0206 - accuracy: 0.2296\n",
            "Epoch 22/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.9238 - accuracy: 0.2366\n",
            "Epoch 23/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.8256 - accuracy: 0.2428\n",
            "Epoch 24/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.7296 - accuracy: 0.2525\n",
            "Epoch 25/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.6312 - accuracy: 0.2630\n",
            "Epoch 26/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.5396 - accuracy: 0.2738\n",
            "Epoch 27/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.4490 - accuracy: 0.2865\n",
            "Epoch 28/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.3637 - accuracy: 0.2985\n",
            "Epoch 29/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.2812 - accuracy: 0.3084\n",
            "Epoch 30/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.2049 - accuracy: 0.3208\n",
            "Epoch 31/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.1333 - accuracy: 0.3315\n",
            "Epoch 32/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.0669 - accuracy: 0.3419\n",
            "Epoch 33/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 3.0026 - accuracy: 0.3508\n",
            "Epoch 34/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.9409 - accuracy: 0.3602\n",
            "Epoch 35/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.8819 - accuracy: 0.3700\n",
            "Epoch 36/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.8246 - accuracy: 0.3799\n",
            "Epoch 37/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.7696 - accuracy: 0.3885\n",
            "Epoch 38/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.7191 - accuracy: 0.3960\n",
            "Epoch 39/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.6683 - accuracy: 0.4051\n",
            "Epoch 40/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.6208 - accuracy: 0.4126\n",
            "Epoch 41/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.5710 - accuracy: 0.4210\n",
            "Epoch 42/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.5270 - accuracy: 0.4298\n",
            "Epoch 43/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.4809 - accuracy: 0.4381\n",
            "Epoch 44/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.4387 - accuracy: 0.4444\n",
            "Epoch 45/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.3953 - accuracy: 0.4526\n",
            "Epoch 46/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.3546 - accuracy: 0.4602\n",
            "Epoch 47/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.3141 - accuracy: 0.4674\n",
            "Epoch 48/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.2785 - accuracy: 0.4745\n",
            "Epoch 49/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.2388 - accuracy: 0.4811\n",
            "Epoch 50/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.2039 - accuracy: 0.4884\n",
            "Epoch 51/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.1693 - accuracy: 0.4941\n",
            "Epoch 52/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.1324 - accuracy: 0.5009\n",
            "Epoch 53/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.0963 - accuracy: 0.5086\n",
            "Epoch 54/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.0656 - accuracy: 0.5152\n",
            "Epoch 55/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.0320 - accuracy: 0.5211\n",
            "Epoch 56/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 2.0015 - accuracy: 0.5270\n",
            "Epoch 57/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.9749 - accuracy: 0.5314\n",
            "Epoch 58/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.9427 - accuracy: 0.5386\n",
            "Epoch 59/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.9156 - accuracy: 0.5436\n",
            "Epoch 60/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.8879 - accuracy: 0.5495\n",
            "Epoch 61/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.8577 - accuracy: 0.5564\n",
            "Epoch 62/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.8302 - accuracy: 0.5625\n",
            "Epoch 63/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.8003 - accuracy: 0.5687\n",
            "Epoch 64/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.7771 - accuracy: 0.5739\n",
            "Epoch 65/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.7549 - accuracy: 0.5763\n",
            "Epoch 66/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.7340 - accuracy: 0.5812\n",
            "Epoch 67/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.7082 - accuracy: 0.5869\n",
            "Epoch 68/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.6854 - accuracy: 0.5906\n",
            "Epoch 69/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.6636 - accuracy: 0.5970\n",
            "Epoch 70/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.6398 - accuracy: 0.6010\n",
            "Epoch 71/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.6157 - accuracy: 0.6054\n",
            "Epoch 72/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.6027 - accuracy: 0.6086\n",
            "Epoch 73/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.5799 - accuracy: 0.6137\n",
            "Epoch 74/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.5598 - accuracy: 0.6175\n",
            "Epoch 75/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.5401 - accuracy: 0.6235\n",
            "Epoch 76/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.5223 - accuracy: 0.6253\n",
            "Epoch 77/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.5068 - accuracy: 0.6287\n",
            "Epoch 78/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.4853 - accuracy: 0.6336\n",
            "Epoch 79/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.4618 - accuracy: 0.6386\n",
            "Epoch 80/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.4504 - accuracy: 0.6414\n",
            "Epoch 81/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.4338 - accuracy: 0.6438\n",
            "Epoch 82/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.4190 - accuracy: 0.6475\n",
            "Epoch 83/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.3998 - accuracy: 0.6529\n",
            "Epoch 84/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.3852 - accuracy: 0.6545\n",
            "Epoch 85/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.3706 - accuracy: 0.6588\n",
            "Epoch 86/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.3645 - accuracy: 0.6593\n",
            "Epoch 87/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.3467 - accuracy: 0.6632\n",
            "Epoch 88/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.3233 - accuracy: 0.6693\n",
            "Epoch 89/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.3134 - accuracy: 0.6712\n",
            "Epoch 90/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.3046 - accuracy: 0.6726\n",
            "Epoch 91/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.2953 - accuracy: 0.6739\n",
            "Epoch 92/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.2765 - accuracy: 0.6778\n",
            "Epoch 93/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.2563 - accuracy: 0.6833\n",
            "Epoch 94/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.2567 - accuracy: 0.6817\n",
            "Epoch 95/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.2442 - accuracy: 0.6847\n",
            "Epoch 96/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.2276 - accuracy: 0.6894\n",
            "Epoch 97/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.2174 - accuracy: 0.6916\n",
            "Epoch 98/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.2049 - accuracy: 0.6939\n",
            "Epoch 99/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.1941 - accuracy: 0.6965\n",
            "Epoch 100/100\n",
            "396/396 [==============================] - 7s 18ms/step - loss: 1.1915 - accuracy: 0.6973\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc841ea990>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# compile model\n",
        "GRU_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "GRU_model.fit(X, y, batch_size=300, epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BQImSJfndBQ"
      },
      "source": [
        "### **Save Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBEb9NovndBR"
      },
      "source": [
        "At the end of the run, the trained model is saved to file.\n",
        "\n",
        "Here, we use the Keras model API to save the model to the file ‘model.h5‘ in the current working directory.\n",
        "\n",
        "Later, when we load the model to make predictions, we will also need the mapping of words to integers. This is in the Tokenizer object, and we can save that too using Pickle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJkVN3B_ndBT"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "# save the model to file\n",
        "GRU_model.save('GRU_model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT3yB2M52SoH"
      },
      "source": [
        "## **Fit BI-Directional LSTM Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC8sYKJT91HF"
      },
      "source": [
        "We can now define and fit our language model on the training data.\n",
        "\n",
        "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
        "\n",
        "\n",
        "We will use a two LSTM and BI-Directional LSTM  hidden layers  with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
        "\n",
        "A dense fully connected layer with 300 neurons connects to the BI-Directional LSTM hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRCSfiSK2MmY"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "BI_LSTM_model = Sequential()\n",
        "BI_LSTM_model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "BI_LSTM_model.add(LSTM(100, return_sequences=True))\n",
        "BI_LSTM_model.add( Bidirectional (LSTM(100)))\n",
        "BI_LSTM_model.add(Dense(300, activation='relu'))\n",
        "BI_LSTM_model.add(Dense(vocab_size, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1v8E1zvAZcn"
      },
      "source": [
        "A summary of the defined network is printed as a sanity check to ensure we have constructed what we intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVRESy2GAcNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c4ce68-c4ca-4488-b1f0-11e8a7704e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 50)            370500    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 100)           60400     \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 200)              160800    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 300)               60300     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7410)              2230410   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,882,410\n",
            "Trainable params: 2,882,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(BI_LSTM_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4l5UYbPAhGV"
      },
      "source": [
        "Next, the model is compiled specifying the categorical cross entropy loss needed to fit the model. Technically, the model is learning a multi-class classification and this is the suitable loss function for this type of problem. The efficient Adam implementation to mini-batch gradient descent is used and accuracy is evaluated of the model.\n",
        "\n",
        "Finally, the model is fit on the data for 100 training epochs with a modest batch size of 300 to speed things up.\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79ReZLIv3QKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35d2ff9-739a-462d-8145-3a38e9df83fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "396/396 [==============================] - 17s 24ms/step - loss: 6.2740 - accuracy: 0.0618\n",
            "Epoch 2/100\n",
            "396/396 [==============================] - 10s 24ms/step - loss: 5.8455 - accuracy: 0.0927\n",
            "Epoch 3/100\n",
            "396/396 [==============================] - 10s 24ms/step - loss: 5.6362 - accuracy: 0.1106\n",
            "Epoch 4/100\n",
            "396/396 [==============================] - 10s 24ms/step - loss: 5.4559 - accuracy: 0.1314\n",
            "Epoch 5/100\n",
            "396/396 [==============================] - 10s 24ms/step - loss: 5.3257 - accuracy: 0.1424\n",
            "Epoch 6/100\n",
            "396/396 [==============================] - 10s 24ms/step - loss: 5.2277 - accuracy: 0.1490\n",
            "Epoch 7/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 5.1543 - accuracy: 0.1530\n",
            "Epoch 8/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 5.0728 - accuracy: 0.1589\n",
            "Epoch 9/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 5.0036 - accuracy: 0.1627\n",
            "Epoch 10/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.9424 - accuracy: 0.1649\n",
            "Epoch 11/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.8830 - accuracy: 0.1674\n",
            "Epoch 12/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.8203 - accuracy: 0.1701\n",
            "Epoch 13/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.7584 - accuracy: 0.1723\n",
            "Epoch 14/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.6874 - accuracy: 0.1740\n",
            "Epoch 15/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.6136 - accuracy: 0.1775\n",
            "Epoch 16/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.5399 - accuracy: 0.1797\n",
            "Epoch 17/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.4631 - accuracy: 0.1827\n",
            "Epoch 18/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.3955 - accuracy: 0.1847\n",
            "Epoch 19/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.3183 - accuracy: 0.1883\n",
            "Epoch 20/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.2323 - accuracy: 0.1926\n",
            "Epoch 21/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.1476 - accuracy: 0.1988\n",
            "Epoch 22/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 4.0606 - accuracy: 0.2062\n",
            "Epoch 23/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.9796 - accuracy: 0.2140\n",
            "Epoch 24/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.9004 - accuracy: 0.2225\n",
            "Epoch 25/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.8267 - accuracy: 0.2316\n",
            "Epoch 26/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.7557 - accuracy: 0.2398\n",
            "Epoch 27/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.6881 - accuracy: 0.2481\n",
            "Epoch 28/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.6224 - accuracy: 0.2557\n",
            "Epoch 29/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.5700 - accuracy: 0.2634\n",
            "Epoch 30/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.5076 - accuracy: 0.2702\n",
            "Epoch 31/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.4464 - accuracy: 0.2786\n",
            "Epoch 32/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.3866 - accuracy: 0.2878\n",
            "Epoch 33/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.3282 - accuracy: 0.2955\n",
            "Epoch 34/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.2755 - accuracy: 0.3026\n",
            "Epoch 35/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.2221 - accuracy: 0.3100\n",
            "Epoch 36/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.1710 - accuracy: 0.3173\n",
            "Epoch 37/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.1217 - accuracy: 0.3255\n",
            "Epoch 38/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.0728 - accuracy: 0.3326\n",
            "Epoch 39/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 3.0265 - accuracy: 0.3394\n",
            "Epoch 40/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.9806 - accuracy: 0.3461\n",
            "Epoch 41/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.9347 - accuracy: 0.3539\n",
            "Epoch 42/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.8925 - accuracy: 0.3615\n",
            "Epoch 43/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.8484 - accuracy: 0.3685\n",
            "Epoch 44/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.8075 - accuracy: 0.3754\n",
            "Epoch 45/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.7645 - accuracy: 0.3811\n",
            "Epoch 46/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.7243 - accuracy: 0.3887\n",
            "Epoch 47/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.6816 - accuracy: 0.3966\n",
            "Epoch 48/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.6472 - accuracy: 0.4018\n",
            "Epoch 49/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.6073 - accuracy: 0.4093\n",
            "Epoch 50/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.5692 - accuracy: 0.4164\n",
            "Epoch 51/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.5341 - accuracy: 0.4217\n",
            "Epoch 52/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.5004 - accuracy: 0.4285\n",
            "Epoch 53/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.4619 - accuracy: 0.4351\n",
            "Epoch 54/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.4285 - accuracy: 0.4418\n",
            "Epoch 55/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.3946 - accuracy: 0.4488\n",
            "Epoch 56/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.3638 - accuracy: 0.4549\n",
            "Epoch 57/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.3322 - accuracy: 0.4596\n",
            "Epoch 58/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.3007 - accuracy: 0.4665\n",
            "Epoch 59/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.2691 - accuracy: 0.4729\n",
            "Epoch 60/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.2384 - accuracy: 0.4791\n",
            "Epoch 61/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.2081 - accuracy: 0.4854\n",
            "Epoch 62/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.1774 - accuracy: 0.4909\n",
            "Epoch 63/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.1494 - accuracy: 0.4959\n",
            "Epoch 64/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.1223 - accuracy: 0.5007\n",
            "Epoch 65/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.0903 - accuracy: 0.5087\n",
            "Epoch 66/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.0633 - accuracy: 0.5144\n",
            "Epoch 67/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 2.0359 - accuracy: 0.5188\n",
            "Epoch 68/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 2.0081 - accuracy: 0.5242\n",
            "Epoch 69/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.9797 - accuracy: 0.5305\n",
            "Epoch 70/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.9550 - accuracy: 0.5351\n",
            "Epoch 71/100\n",
            "396/396 [==============================] - 11s 28ms/step - loss: 1.9271 - accuracy: 0.5401\n",
            "Epoch 72/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.9020 - accuracy: 0.5447\n",
            "Epoch 73/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.8768 - accuracy: 0.5514\n",
            "Epoch 74/100\n",
            "396/396 [==============================] - 11s 27ms/step - loss: 1.8498 - accuracy: 0.5560\n",
            "Epoch 75/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.8236 - accuracy: 0.5619\n",
            "Epoch 76/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.7997 - accuracy: 0.5663\n",
            "Epoch 77/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.7753 - accuracy: 0.5722\n",
            "Epoch 78/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.7511 - accuracy: 0.5784\n",
            "Epoch 79/100\n",
            "396/396 [==============================] - 11s 28ms/step - loss: 1.7263 - accuracy: 0.5823\n",
            "Epoch 80/100\n",
            "396/396 [==============================] - 12s 31ms/step - loss: 1.7022 - accuracy: 0.5877\n",
            "Epoch 81/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.6796 - accuracy: 0.5932\n",
            "Epoch 82/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.6557 - accuracy: 0.5989\n",
            "Epoch 83/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.6335 - accuracy: 0.6031\n",
            "Epoch 84/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.6085 - accuracy: 0.6089\n",
            "Epoch 85/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.5904 - accuracy: 0.6115\n",
            "Epoch 86/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.5660 - accuracy: 0.6164\n",
            "Epoch 87/100\n",
            "396/396 [==============================] - 11s 27ms/step - loss: 1.5424 - accuracy: 0.6223\n",
            "Epoch 88/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.5213 - accuracy: 0.6280\n",
            "Epoch 89/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.5010 - accuracy: 0.6318\n",
            "Epoch 90/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.4820 - accuracy: 0.6354\n",
            "Epoch 91/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.4598 - accuracy: 0.6406\n",
            "Epoch 92/100\n",
            "396/396 [==============================] - 10s 26ms/step - loss: 1.4386 - accuracy: 0.6459\n",
            "Epoch 93/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.4156 - accuracy: 0.6519\n",
            "Epoch 94/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.3978 - accuracy: 0.6550\n",
            "Epoch 95/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.3750 - accuracy: 0.6600\n",
            "Epoch 96/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.3554 - accuracy: 0.6645\n",
            "Epoch 97/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.3347 - accuracy: 0.6695\n",
            "Epoch 98/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.3193 - accuracy: 0.6723\n",
            "Epoch 99/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.3021 - accuracy: 0.6770\n",
            "Epoch 100/100\n",
            "396/396 [==============================] - 10s 25ms/step - loss: 1.2815 - accuracy: 0.6812\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5765488950>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# compile model\n",
        "BI_LSTM_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "BI_LSTM_model.fit(X, y, batch_size=300, epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdzLBrTUAw5o"
      },
      "source": [
        "After we run our BI-Directional LSTM model, we observed that an accuracy increases for each epoch so we can increase the number of epochs to make good prediction for the next word in the sequence, which is not bad. We are not aiming for 100% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSQfdnkf3nU3"
      },
      "source": [
        "### **Save Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8R2cxs8A4S7"
      },
      "source": [
        "At the end of the run, the trained model is saved to file.\n",
        "\n",
        "Here, we use the Keras model API to save the model to the file ‘model.h5‘ in the current working directory.\n",
        "\n",
        "Later, when we load the model to make predictions, we will also need the mapping of words to integers. This is in the Tokenizer object, and we can save that too using Pickle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbaup0Sn3Y0i"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "# save the model to file\n",
        "BI_LSTM_model.save('BI_LSTM_model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clear session\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "32JbjoAk_mzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPdku24_ia0o"
      },
      "source": [
        "## **Fit BI-Directional GRU Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZj6FbDuia0p"
      },
      "source": [
        "We can now define and fit our language model on the training data.\n",
        "\n",
        "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
        "\n",
        "\n",
        "We will use a two gru and Bidirectional hidden layers with 128 and 64 memory cells each. More memory cells and a deeper network may achieve better results.\n",
        "\n",
        "A dense fully connected layer with 300 neurons connects to the bidirectional hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl5QCevHia0r"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "BI_GRU_model = Sequential( )\n",
        "BI_GRU_model.add (Embedding(vocab_size , 50 , input_length = seq_length ))\n",
        "BI_GRU_model.add (GRU(128 , return_sequences = True )) \n",
        "BI_GRU_model.add (Bidirectional (GRU(64))) \n",
        "BI_GRU_model.add (Dense (300 , activation = 'relu'))\n",
        "BI_GRU_model.add (Dense(vocab_size , activation = 'softmax')) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj9G2p0aia0t"
      },
      "source": [
        "A summary of the defined network is printed as a sanity check to ensure we have constructed what we intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d05b33e-0dd2-455f-cfd5-823b24b97a8b",
        "id": "n56lEow3ia0u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 50, 50)            370500    \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 50, 128)           69120     \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              74496     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 300)               38700     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7410)              2230410   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,783,226\n",
            "Trainable params: 2,783,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(BI_GRU_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx6ntPTzia0v"
      },
      "source": [
        "Next, the model is compiled specifying the categorical cross entropy loss needed to fit the model. Technically, the model is learning a multi-class classification and this is the suitable loss function for this type of problem. The efficient Adam implementation to mini-batch gradient descent is used and accuracy is evaluated of the model.\n",
        "\n",
        "Finally, the model is fit on the data for 100 training epochs with a modest batch size of 128 to speed things up.\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95dff282-a60d-428e-b249-b538b0711062",
        "id": "L8HaDK1Wia0w"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "396/396 [==============================] - 17s 21ms/step - loss: 6.3507 - accuracy: 0.0580\n",
            "Epoch 2/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 5.9909 - accuracy: 0.0743\n",
            "Epoch 3/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 5.5265 - accuracy: 0.1241\n",
            "Epoch 4/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 5.1849 - accuracy: 0.1505\n",
            "Epoch 5/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 4.9524 - accuracy: 0.1638\n",
            "Epoch 6/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 4.7605 - accuracy: 0.1760\n",
            "Epoch 7/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 4.5845 - accuracy: 0.1856\n",
            "Epoch 8/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 4.4081 - accuracy: 0.1938\n",
            "Epoch 9/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 4.2269 - accuracy: 0.2024\n",
            "Epoch 10/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 4.0331 - accuracy: 0.2110\n",
            "Epoch 11/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 3.8286 - accuracy: 0.2248\n",
            "Epoch 12/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 3.6169 - accuracy: 0.2504\n",
            "Epoch 13/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 3.4073 - accuracy: 0.2782\n",
            "Epoch 14/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 3.2129 - accuracy: 0.3081\n",
            "Epoch 15/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 3.0367 - accuracy: 0.3335\n",
            "Epoch 16/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 2.8782 - accuracy: 0.3581\n",
            "Epoch 17/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 2.7345 - accuracy: 0.3820\n",
            "Epoch 18/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 2.6041 - accuracy: 0.4027\n",
            "Epoch 19/100\n",
            "396/396 [==============================] - 9s 21ms/step - loss: 2.4868 - accuracy: 0.4242\n",
            "Epoch 20/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 2.3764 - accuracy: 0.4443\n",
            "Epoch 21/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 2.2770 - accuracy: 0.4624\n",
            "Epoch 22/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 2.1813 - accuracy: 0.4806\n",
            "Epoch 23/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 2.0929 - accuracy: 0.4988\n",
            "Epoch 24/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 2.0100 - accuracy: 0.5137\n",
            "Epoch 25/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.9321 - accuracy: 0.5302\n",
            "Epoch 26/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.8546 - accuracy: 0.5471\n",
            "Epoch 27/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 1.7834 - accuracy: 0.5608\n",
            "Epoch 28/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 1.7135 - accuracy: 0.5768\n",
            "Epoch 29/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.6467 - accuracy: 0.5919\n",
            "Epoch 30/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.5830 - accuracy: 0.6055\n",
            "Epoch 31/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.5209 - accuracy: 0.6197\n",
            "Epoch 32/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.4610 - accuracy: 0.6335\n",
            "Epoch 33/100\n",
            "396/396 [==============================] - 9s 21ms/step - loss: 1.4030 - accuracy: 0.6453\n",
            "Epoch 34/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 1.3463 - accuracy: 0.6596\n",
            "Epoch 35/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.2919 - accuracy: 0.6720\n",
            "Epoch 36/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.2364 - accuracy: 0.6862\n",
            "Epoch 37/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.1873 - accuracy: 0.6984\n",
            "Epoch 38/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.1378 - accuracy: 0.7086\n",
            "Epoch 39/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.0889 - accuracy: 0.7215\n",
            "Epoch 40/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 1.0447 - accuracy: 0.7316\n",
            "Epoch 41/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.9997 - accuracy: 0.7428\n",
            "Epoch 42/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 0.9548 - accuracy: 0.7551\n",
            "Epoch 43/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.9135 - accuracy: 0.7649\n",
            "Epoch 44/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.8714 - accuracy: 0.7736\n",
            "Epoch 45/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.8349 - accuracy: 0.7846\n",
            "Epoch 46/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.7976 - accuracy: 0.7931\n",
            "Epoch 47/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.7603 - accuracy: 0.8027\n",
            "Epoch 48/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.7249 - accuracy: 0.8125\n",
            "Epoch 49/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 0.6895 - accuracy: 0.8218\n",
            "Epoch 50/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.6569 - accuracy: 0.8302\n",
            "Epoch 51/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 0.6287 - accuracy: 0.8363\n",
            "Epoch 52/100\n",
            "396/396 [==============================] - 9s 21ms/step - loss: 0.5981 - accuracy: 0.8448\n",
            "Epoch 53/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.5697 - accuracy: 0.8521\n",
            "Epoch 54/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.5392 - accuracy: 0.8611\n",
            "Epoch 55/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.5144 - accuracy: 0.8665\n",
            "Epoch 56/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.4916 - accuracy: 0.8723\n",
            "Epoch 57/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.4674 - accuracy: 0.8783\n",
            "Epoch 58/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.4484 - accuracy: 0.8829\n",
            "Epoch 59/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.4285 - accuracy: 0.8884\n",
            "Epoch 60/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.4089 - accuracy: 0.8933\n",
            "Epoch 61/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.3826 - accuracy: 0.9014\n",
            "Epoch 62/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.3640 - accuracy: 0.9061\n",
            "Epoch 63/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.3485 - accuracy: 0.9094\n",
            "Epoch 64/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.3362 - accuracy: 0.9129\n",
            "Epoch 65/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.3283 - accuracy: 0.9134\n",
            "Epoch 66/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.3151 - accuracy: 0.9160\n",
            "Epoch 67/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2971 - accuracy: 0.9217\n",
            "Epoch 68/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2747 - accuracy: 0.9288\n",
            "Epoch 69/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2574 - accuracy: 0.9341\n",
            "Epoch 70/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2583 - accuracy: 0.9322\n",
            "Epoch 71/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 0.2635 - accuracy: 0.9287\n",
            "Epoch 72/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2535 - accuracy: 0.9307\n",
            "Epoch 73/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2336 - accuracy: 0.9382\n",
            "Epoch 74/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2124 - accuracy: 0.9446\n",
            "Epoch 75/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2045 - accuracy: 0.9462\n",
            "Epoch 76/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2128 - accuracy: 0.9430\n",
            "Epoch 77/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2249 - accuracy: 0.9372\n",
            "Epoch 78/100\n",
            "396/396 [==============================] - 8s 21ms/step - loss: 0.2102 - accuracy: 0.9424\n",
            "Epoch 79/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1845 - accuracy: 0.9508\n",
            "Epoch 80/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1591 - accuracy: 0.9603\n",
            "Epoch 81/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1555 - accuracy: 0.9609\n",
            "Epoch 82/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1864 - accuracy: 0.9475\n",
            "Epoch 83/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2150 - accuracy: 0.9366\n",
            "Epoch 84/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2006 - accuracy: 0.9411\n",
            "Epoch 85/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1548 - accuracy: 0.9592\n",
            "Epoch 86/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1263 - accuracy: 0.9685\n",
            "Epoch 87/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1204 - accuracy: 0.9703\n",
            "Epoch 88/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1516 - accuracy: 0.9580\n",
            "Epoch 89/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2228 - accuracy: 0.9309\n",
            "Epoch 90/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2095 - accuracy: 0.9355\n",
            "Epoch 91/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1438 - accuracy: 0.9597\n",
            "Epoch 92/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.0837 - accuracy: 0.9825\n",
            "Epoch 93/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.0512 - accuracy: 0.9931\n",
            "Epoch 94/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.0368 - accuracy: 0.9964\n",
            "Epoch 95/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.0304 - accuracy: 0.9976\n",
            "Epoch 96/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.1930 - accuracy: 0.9455\n",
            "Epoch 97/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.6870 - accuracy: 0.7974\n",
            "Epoch 98/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.2341 - accuracy: 0.9258\n",
            "Epoch 99/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.0857 - accuracy: 0.9805\n",
            "Epoch 100/100\n",
            "396/396 [==============================] - 9s 22ms/step - loss: 0.0371 - accuracy: 0.9966\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd6b6588f50>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# compile model\n",
        "BI_GRU_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "BI_GRU_model.fit(X, y, batch_size=300 , epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCkjy_PSia0x"
      },
      "source": [
        "After we run our BI directional GRU model, we observed that an accuracy increases for each epoch so we can increase the number of epochs to make good prediction for the next word in the sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-jkSOryia0y"
      },
      "source": [
        "### **Save Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK6V_JDSia0z"
      },
      "source": [
        "At the end of the run, the trained model is saved to file.\n",
        "\n",
        "Here, we use the Keras model API to save the model to the file ‘model.h5‘ in the current working directory.\n",
        "\n",
        "Later, when we load the model to make predictions, we will also need the mapping of words to integers. This is in the Tokenizer object, and we can save that too using Pickle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg2o4wi-ia00"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "# save the model to file\n",
        "BI_GRU_model.save('BI_GRU_model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PFdlBHH4pbs"
      },
      "source": [
        "# **Use Language Model**\n",
        "Now that we have a trained language model, we can use it.\n",
        "\n",
        "In this case, we can use it to generate new sequences of text that have the same statistical properties as the source text.\n",
        "\n",
        "\n",
        "This is not practical, at least not for this example, but it gives a concrete example of what the language model has learned.\n",
        "\n",
        "\n",
        "We will start by loading the training sequences again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rASVM7-6U6T"
      },
      "source": [
        "# **Generate Text**\n",
        "The first step in generating text is preparing a seed input.\n",
        "\n",
        "We will select a random line of text from the input text for this purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n-gEwJr7E0J"
      },
      "source": [
        "We can create a function called generate_seq() that takes as input the model, the tokenizer, input sequence length, the seed text, and the number of words to generate. It then returns a sequence of words generated by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pKW8PO46-ND"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\t# yhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\tyhat = np.argmax(model.predict(np.array(encoded)), axis=1)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "seq_length = len(lines[0].split()) - 1\n",
        " \n",
        "# load the model\n",
        "LSTM_model  = load_model('LSTM_model.h5')\n",
        "GRU_model   = load_model('GRU_model.h5')\n",
        "BI_LSTM_modeL = load_model('BI_LSTM_modeL.h5')\n",
        "BI_GRU_model = load_model('BI_GRU_model.h5')\n",
        " \n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        " \n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPGNPgY2nq31",
        "outputId": "6b59cdd4-965c-42bc-9814-7fed30887a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slavery more than death undoubtedly also we shall have to reject all the terrible and appalling names which describe the world below cocytus and styx ghosts under the earth and sapless shades and any similar words of which the very mention causes a shudder to pass through the inmost soul of\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkbvzzgo7KKE"
      },
      "source": [
        "We are now ready to generate a sequence of new words given some seed text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpAOXfir7LnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f1d98da-e066-4787-efd5-3b94f483c85a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to be attributed to their fine wits time to the islands of the blest and dwell and of the whole state and the meaner perfect is the sun then we shall be satisfied with that the eye is not the penalties and retributions and they will erase and another man\n"
          ]
        }
      ],
      "source": [
        "# generate new text\n",
        "generated = generate_seq(LSTM_model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate new text\n",
        "generated = generate_seq(GRU_model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "fu-tW751sZwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef1f10fa-2d35-4fd1-a8f5-d26054891241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not the several persons who are heard in the institution of gymnastic also rather than themselves and that he dragged even to range in the view or themselves and to be compared with the world only to his custom and not be punished but when he has a lover of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate new text\n",
        "generated = generate_seq(BI_LSTM_modeL, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "LeVl62HZscTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07af875-141b-4914-94b7-6fe4afe613b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extremely called the authors of the good vary but who has him persuade before them therefore they are seeking not useless with tools or the injustice of plunder yes he said i expect that those and educators which are the distance and not therefore you have the most aggravated city\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate new text\n",
        "generated = generate_seq(BI_GRU_model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "OEiVXi-LsyZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70626246-4c36-46fe-f23f-a35b585e1caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not the happier which the embroiderer what you say i cannot examine giving those be or as our state which is the question which i will to reflect what do you say of the situation have the same object of repose you mean to hear the informer of democracy we\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YXFLoPEB836"
      },
      "source": [
        "**Note**: The results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision.\n",
        "\n",
        "we can see that the text seems reasonable. In fact, the addition of concatenation would help in interpreting the seed and the generated text. Nevertheless, the generated text gets the right kind of words in the right kind of order.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-Hgzvmjndhwo",
        "FL72iL44yNLu",
        "U1IYn9p8yclh",
        "q0-PE6Bty0-W",
        "774BvdCAzaps",
        "k2bXsWYB1pp1",
        "GYK6Uzip9WWk",
        "OZld9HlVndBB",
        "KT3yB2M52SoH",
        "MPdku24_ia0o",
        "6PFdlBHH4pbs"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}