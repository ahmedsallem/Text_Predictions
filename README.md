# Text_Predictions

Building a Recurrent Language Model 

A language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence.
Neural network models are a preferred method for developing statistical language models because they can use a distributed representation where different words with similar meanings have similar representation and because they can use a large context of recently observed words when making predictions.
# Data source:

We will use The Republic by Plato as the source text that we can found it by using https://www.gutenberg.org/cache/epub/1497/pg1497.txt. But in this file, we will remove the front and back matter. This includes details about the book at the beginning, a long analysis, and license information at the end. 


## models that used:

LSTM Model

GRU Model

BI-Directional LSTM Model

BI-Directional GRU Model

